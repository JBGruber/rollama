@misc{weber2023evaluation,
	title        = {Evaluation is all you need. Prompting Generative Large Language Models for Annotation Tasks in the Social Sciences. A Primer using Open Models},
	author       = {Maximilian Weber and Merle Reichardt},
	year         = 2023,
	eprint       = {2401.00284},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{Chae_Davidson_2023,
	title        = {Large Language Models for Text Classification: From Zero-Shot Learning to Fine-Tuning},
	author       = {Chae, Youngjin (YJ) and Davidson, Thomas},
	year         = 2023,
	publisher    = {OSF},
	doi          = {10.31235/osf.io/sthwk},
	url          = {https://osf.io/sthwk},
	language     = {en-us}
}
@misc{Davidson_2023,
	title        = {Start Generating: Harnessing Generative Artificial Intelligence for Sociological Research},
	author       = {Davidson, Thomas},
	year         = 2023,
	publisher    = {OSF},
	doi          = {10.31235/osf.io/u9nft},
	url          = {https://osf.io/u9nft},
	language     = {en-us}
}
@article{GilardiChatGPT2023,
	title        = {ChatGPT outperforms crowd workers for text-annotation tasks},
	author       = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	year         = 2023,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {Proceedings of the National Academy of Sciences},
	volume       = 120,
	number       = 30,
	doi          = {10.1073/pnas.2305016120},
	url          = {https://www.pnas.org/doi/10.1073/pnas.2305016120}
}
@misc{He_Lin_et_al._2023,
	title        = {AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators},
	author       = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A.-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2303.16854}
}
@misc{Heseltine_Hohenberg_2023,
	title        = {Large Language Models as a Substitute for Human Experts in Annotating Political Text},
	author       = {Heseltine, Michael and Hohenberg, Bernhard Clemm von},
	year         = 2023,
	publisher    = {OSF},
	doi          = {10.31219/osf.io/cx752},
	url          = {https://osf.io/cx752},
	language     = {en-us}
}
@misc{Kheiri_Karimi_2023,
	title        = {SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning},
	author       = {Kheiri, Kiana and Karimi, Hamid},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2307.10234},
	language     = {en}
}
@misc{Kojima_Gu_et_al._2023,
	title        = {Large Language Models are Zero-Shot Reasoners},
	author       = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2205.11916}
}
@misc{Luccioni_Jernite_et_al._2023,
	title        = {Power Hungry Processing: Watts Driving the Cost of AI Deployment?},
	author       = {Luccioni, Alexandra Sasha and Jernite, Yacine and Strubell, Emma},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2311.16863}
}
@misc{Møller_Dalsgaard_et_al._2023,
	title        = {Is a prompt and a few samples all you need? Using GPT-4 for data augmentation in low-resource classification tasks},
	author       = {Møller, Anders Giovanni and Dalsgaard, Jacob Aarup and Pera, Arianna and Aiello, Luca Maria},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2304.13861}
}
@misc{Ollion_Shen_et_al._2023,
	title        = {ChatGPT for Text Annotation? Mind the Hype!},
	author       = {Ollion, Etienne and Shen, Rubing and Macanovic, Ana and Chatelain, Arnault},
	year         = 2023,
	publisher    = {OSF},
	doi          = {10.31235/osf.io/x58kn},
	url          = {https://osf.io/x58kn},
	language     = {en-us}
}
@misc{Pangakis_Wolken_et_al._2023,
	title        = {Automated Annotation with Generative AI Requires Validation},
	author       = {Pangakis, Nicholas and Wolken, Samuel and Fasching, Neil},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2306.00176}
}
@misc{Plaza-del-Arco_Nozza_et_al._2023,
	title        = {Leveraging Label Variation in Large Language Models for Zero-Shot Text Classification},
	author       = {Plaza-del-Arco, Flor Miriam and Nozza, Debora and Hovy, Dirk},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2307.12973}
}
@inproceedings{Rosenthal_Farra_et_al._2017,
	title        = {SemEval-2017 Task 4: Sentiment Analysis in Twitter},
	author       = {Rosenthal, Sara and Farra, Noura and Nakov, Preslav},
	year         = 2017,
	booktitle    = {Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
	publisher    = {Association for Computational Linguistics},
	address      = {Vancouver, Canada},
	pages        = {502–518},
	doi          = {10.18653/v1/S17-2088},
	url          = {https://aclanthology.org/S17-2088}
}
@article{Spirling_2023,
	title        = {Why open-source generative AI models are an ethical way forward for science},
	author       = {Spirling, Arthur},
	year         = 2023,
	journal      = {Nature},
	volume       = 616,
	number       = 7957,
	pages        = {413–413},
	doi          = {10.1038/d41586-023-01295-4},
	rights       = {2023 Springer Nature Limited},
	language     = {en}
}
@misc{Törnberg_2023,
	title        = {ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning},
	author       = {Törnberg, Petter},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2304.06588}
}
@misc{Wang_Wei_et_al._2023,
	title        = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
	author       = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2203.11171}
}
@misc{Wei_Wang_et_al._2023,
	title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	author       = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2201.11903}
}
@misc{White_Fu_et_al._2023,
	title        = {A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT},
	author       = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2302.11382}
}
@misc{Zhu_Zhang_et_al._2023,
	title        = {Can ChatGPT Reproduce Human-Generated Labels? A Study of Social Computing Tasks},
	author       = {Zhu, Yiming and Zhang, Peixian and Haq, Ehsan-Ul and Hui, Pan and Tyson, Gareth},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2304.10145}
}
@misc{Ziems_Held_et_al._2023,
	title        = {Can Large Language Models Transform Computational Social Science?},
	author       = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2305.03514}
}
@inproceedings{Huang_Kwak_et_al._2023,
	title        = {Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech},
	author       = {Huang, Fan and Kwak, Haewoon and An, Jisun},
	year         = 2023,
	booktitle    = {Companion Proceedings of the ACM Web Conference 2023},
	pages        = {294–297},
	doi          = {10.1145/3543873.3587368},
	url          = {http://arxiv.org/abs/2302.07736}
}
@misc{Zhong_ChatGPT_2023,
	title        = {Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT},
	author       = {Zhong, Qihuang and Ding, Liang and Liu, Juhua and Du, Bo and Tao, Dacheng},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2302.10198}
}
@misc{Reiss_2023,
	title        = {Testing the Reliability of ChatGPT for Text Annotation and Classification: A Cautionary Remark},
	author       = {Reiss, Michael V.},
	year         = 2023,
	publisher    = {arXiv},
	url          = {http://arxiv.org/abs/2304.11085}
}
@article{Linegar_Kocielnik_et_al._2023,
	title        = {Large language models and political science},
	author       = {Linegar, Mitchell and Kocielnik, Rafal and Alvarez, R. Michael},
	year         = 2023,
	journal      = {Frontiers in Political Science},
	volume       = 5,
	issn         = {2673-3145},
	url          = {https://www.frontiersin.org/articles/10.3389/fpos.2023.1257092}
}
@misc{Thalken_Stiglitz_Mimno_Wilkens_2023,
	title        = {Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement},
	author       = {Thalken, Rosamond and Stiglitz, Edward H. and Mimno, David and Wilkens, Matthew},
	year         = 2023,
	publisher    = {arXiv},
	number       = {arXiv:2310.18440},
	url          = {http://arxiv.org/abs/2310.18440},
	note         = {arXiv:2310.18440 [cs]}
}
@article{Wankmüller_2022,
	title        = {Introduction to Neural Transfer Learning With Transformers for Social Science Text Analysis},
	author       = {Wankmüller, Sandra},
	year         = 2022,
	journal      = {Sociological Methods \& Research},
	publisher    = {SAGE Publications Inc},
	pages        = {1--77},
	doi          = {10.1177/00491241221134527},
	issn         = {0049-1241},
	abstractnote = {Transformer-based models for transfer learning have the potential to achieve high prediction accuracies on text-based supervised learning tasks with relatively few training data instances. These models are thus likely to benefit social scientists that seek to have as accurate as possible text-based measures, but only have limited resources for annotating training data. To enable social scientists to leverage these potential benefits for their research, this article explains how these methods work, why they might be advantageous, and what their limitations are. Additionally, three Transformer-based models for transfer learning, BERT, RoBERTa, and the Longformer, are compared to conventional machine learning algorithms on three applications. Across all evaluated tasks, textual styles, and training data set sizes, the conventional models are consistently outperformed by transfer learning with Transformers, thereby demonstrating the benefits these models can bring to text-based social science research.},
	language     = {en}
}
@misc{UKDA-SN-5790-2,
	title        = {National Child Development Study: Age 11, Sweep 2, Sample of Essays, 1969},
	author       = {{University of London, Institute of Education, Centre for Longitudinal Studies}},
	year         = 2023,
	publisher    = {UK Data Service},
	doi          = {10.5255/UKDA-SN-5790-2},
	url          = {http://doi.org/10.5255/UKDA-SN-5790-2},
	note         = {SN: 5790}
}

@misc{alizadeh2023opensource,
	title        = {Open-Source Large Language Models Outperform Crowd Workers and Approach ChatGPT in Text-Annotation Tasks},
	author       = {Meysam Alizadeh and Maël Kubli and Zeynab Samei and Shirin Dehghani and Juan Diego Bermeo and Maria Korobeynikova and Fabrizio Gilardi},
	year         = 2023,
	eprint       = {2307.02179},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
