% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat.r
\name{query}
\alias{query}
\alias{chat}
\title{Chat with a LLM through Ollama}
\usage{
query(q, model = NULL, screen = TRUE, server = NULL)

chat(q, model = NULL, screen = TRUE, server = NULL)
}
\arguments{
\item{q}{the question as a character string or a conversation object.}

\item{model}{which model to use. See \url{https://ollama.ai/library} for options.
Default is "llama2". Set option(rollama_model = "modelname") to change
default for the current session. See \link{pull_model} for more details.}

\item{screen}{Logical. Should the answer be printed to the screen.}

\item{server}{URL to an Ollama server (not the API). Defaults to
"http://localhost:11434".}
}
\value{
an httr2 response
}
\description{
Chat with a LLM through Ollama
}
\details{
\code{query} sends a single question to the API, without knowledge about
previous questions (only the config message is relevant). \code{chat} treats new
messages as part of the same conversation until \link{new_chat} is called.
}
\examples{
\dontrun{
# ask a single question
query("why is the sky blue?")

# hold a conversation
chat("why is the sky blue?")
chat("and how do you know that?")
}
}
