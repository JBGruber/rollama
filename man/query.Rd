% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chat.r
\name{query}
\alias{query}
\alias{chat}
\title{Chat with a LLM through Oolama}
\usage{
query(q, model = NULL, screen = TRUE, server = NULL)

chat(q, model = NULL, screen = TRUE, server = NULL)
}
\arguments{
\item{q}{the question.}

\item{model}{which model to use. See \url{https://ollama.ai/library} for options.
Default is "llama2". Set option(rollama_model = "modelname") to change
default for the current session.}

\item{screen}{Logical. Should the answer be printed to the screen.}

\item{server}{URL to an Oolama server (not the API). Defaults to
"http://localhost:11434".}
}
\value{
an httr2 response
}
\description{
Chat with a LLM through Oolama
}
\details{
\code{query} sends a single question to the API, without knowledge about
previous questions (only the config message is relevant). \code{chat} treats new
messages as part of the same conversation until \link{new_chat} is called.
}
\examples{
\dontrun{
# ask a single question
query("why is the sky blue?")

# hold a conversation
chat("why is the sky blue?")
chat("and how do you know that?")
}
}
