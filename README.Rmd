---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# `rollama` <img src="man/figures/logo.png" align="right" height="138" alt="rollama-logo" />

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![R-CMD-check](https://github.com/JBGruber/rollama/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/JBGruber/rollama/actions/workflows/R-CMD-check.yaml)
[![say-thanks](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/JBGruber)
<!-- badges: end -->

The goal of `rollama` is to wrap the Ollama API, which allows you to run different LLMs locally and create an experience similar to ChatGPT/OpenAI's API.
Ollama is very easy to deploy and handles a huge number of models.
Checkout the project here: <https://github.com/jmorganca/ollama>.


## Installation

You can install the development version of `rollama` from [GitHub](https://github.com/) with:

``` r
# install.packages("remotes")
remotes::install_github("JBGruber/`rollama`")
```

The easiest way to get Ollama itself up and running is through [Docker](https://docs.docker.com/desktop/).
From the command line interface, you can start Ollama locally with one command:

```
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

## Example

The first thing you should do after installation is to pull one of the models from <https://ollama.ai/library>.
By calling `pull_model()` without arguments, you are pulling the (current) default model  --- "llama2 7b":

```{r lib}
library(rollama)
```
```{r eval=FALSE}
pull_model()
```

There are two ways to communicate with the Ollama API.
You can make single requests, which does not store any history and treats each query as the beginning of a new chat:

```{r query}
# ask a single question
query("why is the sky blue?")
```

Or you can use the `chat` function, treats all messages sent during an R session as part of the same conversation:

```{r chat}
# hold a conversation
chat("why is the sky blue?")
chat("and how do you know that?")
```

If you are done with a conversation and want to start a new one, you can do that like so:

```{r new}
new_chat()
```


## Configuration

You can configure the server address, the system prompt and the model used for a query or chat.
If not configured otherwise, `rollama` assumes you are using the default port (11434) of a local instance (http://localhost).
Let's make this explicit by setting the option:

```{r server}
options(rollama_server = "http://localhost:11434")
```

You can change how a model answers by setting a configuration or system message in plain English (or another language supported by the model):

```{r config}
options(rollama_config = "You make answers understandable to a 5 year old")
query("why is the sky blue?")
```

By default, the package uses the "llama2 7B" model. Supported models can be found at <https://ollama.ai/library>. To download a specific model make use of the additional information available in "Tags" <https://ollama.ai/library/mistral/tags>.
Change this via `rollama_model`:

```{r model}
options(rollama_model = "mixtral")
# if you don't have the model yet: pull_model("mixtral")
query("why is the sky blue?")
```

