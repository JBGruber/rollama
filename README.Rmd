---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# `rollama` <img src="man/figures/logo.png" align="right" height="138" alt="rollama-logo" />

<!-- badges: start -->
[![R-CMD-check](https://github.com/JBGruber/rollama/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/JBGruber/rollama/actions/workflows/R-CMD-check.yaml)
[![Codecov test coverage](https://codecov.io/gh/JBGruber/rollama/branch/main/graph/badge.svg)](https://app.codecov.io/gh/JBGruber/rollama?branch=main)
[![CRAN status](https://www.r-pkg.org/badges/version/rollama)](https://CRAN.R-project.org/package=rollama)
[![CRAN_Download_Badge](https://cranlogs.r-pkg.org/badges/grand-total/rollama)](https://cran.r-project.org/package=rollama)
[![arXiv:10.48550/arXiv.2404.07654](https://img.shields.io/badge/DOI-arXiv.2404.07654-B31B1B?logo=arxiv)](https://doi.org/10.48550/arXiv.2404.07654)
[![say-thanks](https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg)](https://saythanks.io/to/JBGruber)
<!-- badges: end -->

The goal of `rollama` is to wrap the Ollama API, which allows you to run different LLMs locally and create an experience similar to ChatGPT/OpenAI's API.
Ollama is very easy to deploy and handles a huge number of models.
Checkout the project here: <https://github.com/ollama/ollama>.


## Installation

You can install this package from CRAN:

``` r
install.packages("rollama")
```

Or you can install the development version  of `rollama` from [GitHub](https://github.com/JBGruber/rollama) with:

``` r
# install.packages("remotes")
remotes::install_github("JBGruber/rollama")
```

The easiest way to get Ollama itself up and running is through [Docker](https://docs.docker.com/desktop/).
From the command line interface, you can start Ollama locally with one command (add `sudo` if `permission denied`):

```sh
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

After restarting, you can run Ollama again with the command  (add `sudo` if `permission denied`):

```sh
docker start ollama
```

Alternatively, you can use the Docker Compose file from [this gist](https://gist.github.com/JBGruber/73f9f49f833c6171b8607b976abc0ddc):

```sh
wget https://gist.githubusercontent.com/JBGruber/73f9f49f833c6171b8607b976abc0ddc/raw/4abf613b60a6e816ef975f31b2758d898baab249/docker-compose.yml
docker-compose up -d
```

If you don't know how to use Docker Compose, you can follow this video:

[![Install Docker on macOS, Windows and Linux](https://img.youtube.com/vi/iMyCdd5nP5U/0.jpg)](https://www.youtube.com/watch?v=iMyCdd5nP5U)

## Example

The first thing you should do after installation is to pull one of the models from <https://ollama.com/library>.
By calling `pull_model()` without arguments, you are pulling the (current) default model  --- "llama3 8b":

```{r lib}
library(rollama)
```
```{r eval=FALSE}
pull_model()
```

There are two ways to communicate with the Ollama API.
You can make single requests, which does not store any history and treats each query as the beginning of a new chat:

```{r query}
# ask a single question
query("why is the sky blue?")
```

Or you can use the `chat` function, treats all messages sent during an R session as part of the same conversation:

```{r chat}
# hold a conversation
chat("why is the sky blue?")
chat("and how do you know that?")
```

If you are done with a conversation and want to start a new one, you can do that like so:

```{r new}
new_chat()
```

## Model parameters

You can set a number of model parameters, either by creating a new model, with a [modelfile](https://jbgruber.github.io/rollama/reference/create_model.html), or by including the parameters in the prompt:

```{r}
query("why is the sky blue?", model_params = list(
  seed = 42,
  temperature = 0,
  num_gpu = 0
))
```

```{r include=FALSE, results='asis'}
l <- readLines("https://raw.githubusercontent.com/ollama/ollama/main/docs/modelfile.md")
s <- grep("#### Valid Parameters and Values", l, fixed = TRUE)
e <- grep("### TEMPLATE", l, fixed = TRUE)
cat(l[s:e - 1], sep = "\n")
```


## Configuration

You can configure the server address, the system prompt and the model used for a query or chat.
If not configured otherwise, `rollama` assumes you are using the default port (11434) of a local instance ("localhost").
Let's make this explicit by setting the option:

```{r server}
options(rollama_server = "http://localhost:11434")
```

You can change how a model answers by setting a configuration or system message in plain English (or another language supported by the model):

```{r config}
options(rollama_config = "You make answers understandable to a 5 year old")
query("why is the sky blue?")
```

By default, the package uses the "llama3 8B" model. Supported models can be found at <https://ollama.com/library>.
To download a specific model make use of the additional information available in "Tags" <https://ollama.com/library/mistral/tags>.
Change this via `rollama_model`:

```{r model}
options(rollama_model = "mixtral")
# if you don't have the model yet: pull_model("mixtral")
query("why is the sky blue?")
```

