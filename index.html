<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Communicate with Ollama to Run Large Language Models Locally • rollama</title>
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Communicate with Ollama to Run Large Language Models Locally">
<meta name="description" content="Wraps the Ollama &lt;https://ollama.com&gt; API, which can be used to communicate with generative large language models locally.">
<meta property="og:description" content="Wraps the Ollama &lt;https://ollama.com&gt; API, which can be used to communicate with generative large language models locally.">
<meta property="og:image" content="https://jbgruber.github.io/rollama/logo.svg">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">rollama</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="articles/annotation.html">annotation</a></li>
    <li><a class="dropdown-item" href="articles/hf-gguf.html">Hugging Face Models</a></li>
    <li><a class="dropdown-item" href="articles/image-annotation.html">image-annotation</a></li>
    <li><a class="dropdown-item" href="articles/text-embedding.html">text-embedding</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/JBGruber/rollama/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header">
<img src="logo.svg" class="logo" alt=""><h1 id="rollama-">
<code>rollama</code> <a class="anchor" aria-label="anchor" href="#rollama-"></a>
</h1>
</div>
<!-- badges: start -->

<p>The goal of <code>rollama</code> is to wrap the Ollama API, which allows you to run different LLMs locally and create an experience similar to ChatGPT/OpenAI’s API. Ollama is very easy to deploy and handles a huge number of models. Checkout the project here: <a href="https://github.com/ollama/ollama" class="external-link uri">https://github.com/ollama/ollama</a>.</p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>You can install this package from CRAN:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"rollama"</span><span class="op">)</span></span></code></pre></div>
<p>Or you can install the development version of <code>rollama</code> from <a href="https://github.com/JBGruber/rollama" class="external-link">GitHub</a>. This version is updated more frequently and may contain bug fixes (or new bugs):</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("remotes")</span></span>
<span><span class="fu">remotes</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"JBGruber/rollama"</span><span class="op">)</span></span></code></pre></div>
<p>However, <code>rollama</code> is just the client package. The models are run in <code>Ollama</code>, which you need to install on your system, on a remote system or through <a href="https://docs.docker.com/desktop/" class="external-link">Docker</a>. The easiest way is to simply download and install the Ollama application from <a href="https://ollama.com/" class="external-link">their website</a>. Once <code>Ollama</code> is running, you can see if you can access it with:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">rollama</span><span class="fu">::</span><span class="fu"><a href="reference/ping_ollama.html">ping_ollama</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; ▶ Ollama (v0.6.1) is running at &lt;http://localhost:11434&gt;!</span></span></code></pre></div>
<div class="section level3">
<h3 id="installation-of-ollama-through-docker">Installation of Ollama through Docker<a class="anchor" aria-label="anchor" href="#installation-of-ollama-through-docker"></a>
</h3>
<p>For beginners we recommend to download Ollama from <a href="https://ollama.com/" class="external-link">their website</a>. However, if you are familiar with Docker, you can also run Ollama through Docker. The advantage of running things through Docker is that the application is isolated from the rest of your system, behaves the same on different systems, and is easy to download and update. You can also get a nice web interface. After making sure <a href="https://docs.docker.com/desktop/" class="external-link">Docker</a> is installed, you can simply use the Docker Compose file from <a href="https://gist.github.com/JBGruber/73f9f49f833c6171b8607b976abc0ddc" class="external-link">this gist</a>.</p>
<p>If you don’t know how to use Docker Compose, you can follow this <a href="https://www.youtube.com/watch?v=iMyCdd5nP5U" class="external-link">video</a> to use the compose file and start Ollama and Open WebUI.</p>
</div>
</div>
<div class="section level2">
<h2 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h2>
<p>The first thing you should do after installation is to pull one of the models from <a href="https://ollama.com/library" class="external-link uri">https://ollama.com/library</a>. By calling <code><a href="reference/pull_model.html">pull_model()</a></code> without arguments, you are pulling the (current) default model — “llama3.1 8b”:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jbgruber.github.io/rollama/">rollama</a></span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/pull_model.html">pull_model</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>There are two ways to communicate with the Ollama API. You can make single requests, which does not store any history and treats each query as the beginning of a new chat:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># ask a single question</span></span>
<span><span class="fu"><a href="reference/query.html">query</a></span><span class="op">(</span><span class="st">"Why is the sky blue? Answer with one sentence."</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.1 ────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; The sky appears blue because of a phenomenon called Rayleigh scattering, in</span></span>
<span><span class="co">#&gt; which shorter (blue) wavelengths of light are scattered more than longer (red)</span></span>
<span><span class="co">#&gt; wavelengths by the tiny molecules of gases in the Earth's atmosphere.</span></span></code></pre></div>
<p>With the output argument, we can specify the format of the response. Available options include “text”, “list”, “data.frame”, “response”, “httr2_response”, and “httr2_request”:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># ask a single question and specify the output format</span></span>
<span><span class="fu"><a href="reference/query.html">query</a></span><span class="op">(</span><span class="st">"Why is the sky blue? Answer with one sentence."</span> , output <span class="op">=</span> <span class="st">"text"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.1 ────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; The sky appears blue because of a phenomenon called Rayleigh scattering, in</span></span>
<span><span class="co">#&gt; which shorter (blue) wavelengths of light are scattered more than longer (red)</span></span>
<span><span class="co">#&gt; wavelengths by the tiny molecules of gases in the Earth's atmosphere.</span></span></code></pre></div>
<p>Or you can use the <code>chat</code> function, treats all messages sent during an R session as part of the same conversation:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># hold a conversation</span></span>
<span><span class="fu"><a href="reference/query.html">chat</a></span><span class="op">(</span><span class="st">"Why is the sky blue? Give a short answer."</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.1 ────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; The sky appears blue because of a phenomenon called Rayleigh scattering, where</span></span>
<span><span class="co">#&gt; shorter (blue) wavelengths of light are scattered more than longer (red)</span></span>
<span><span class="co">#&gt; wavelengths by the tiny molecules of gases in the atmosphere. This scattering</span></span>
<span><span class="co">#&gt; effect gives our sky its distinctive blue color during the daytime.</span></span>
<span><span class="fu"><a href="reference/query.html">chat</a></span><span class="op">(</span><span class="st">"And how do you know that? Give a short answer."</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.1 ────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; I was trained on a vast amount of scientific knowledge and data, including</span></span>
<span><span class="co">#&gt; information from various fields like physics, atmospheric science, and</span></span>
<span><span class="co">#&gt; astronomy. Additionally, I've been fine-tuned to recognize and recall reliable</span></span>
<span><span class="co">#&gt; sources, such as NASA, the Royal Society, and other reputable institutions that</span></span>
<span><span class="co">#&gt; explain the phenomenon of Rayleigh scattering and its effect on the sky's</span></span>
<span><span class="co">#&gt; color.</span></span></code></pre></div>
<p>If you are done with a conversation and want to start a new one, you can do that like so:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/chat_history.html">new_chat</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="model-parameters">Model parameters<a class="anchor" aria-label="anchor" href="#model-parameters"></a>
</h2>
<p>You can set a number of model parameters, either by creating a new model, with a <a href="https://jbgruber.github.io/rollama/reference/create_model.html">modelfile</a>, or by including the parameters in the prompt:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/query.html">query</a></span><span class="op">(</span><span class="st">"Why is the sky blue? Answer with one sentence."</span>, output <span class="op">=</span> <span class="st">"text"</span>,</span>
<span>      model_params <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>        seed <span class="op">=</span> <span class="fl">42</span>,</span>
<span>        num_gpu <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span>      <span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.1 ────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; The sky appears blue because of a phenomenon called Rayleigh scattering, in</span></span>
<span><span class="co">#&gt; which shorter (blue) wavelengths of light are scattered more than longer (red)</span></span>
<span><span class="co">#&gt; wavelengths by the tiny molecules of gases in the Earth's atmosphere.</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="configuration">Configuration<a class="anchor" aria-label="anchor" href="#configuration"></a>
</h2>
<p>You can configure the server address, the system prompt and the model used for a query or chat. If not configured otherwise, <code>rollama</code> assumes you are using the default port (11434) of a local instance (“localhost”). Let’s make this explicit by setting the option:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>rollama_server <span class="op">=</span> <span class="st">"http://localhost:11434"</span><span class="op">)</span></span></code></pre></div>
<p>You can change how a model answers by setting a configuration or system message in plain English (or another language supported by the model):</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>rollama_config <span class="op">=</span> <span class="st">"You make short answers understandable to a 5 year old"</span><span class="op">)</span></span>
<span><span class="fu"><a href="reference/query.html">query</a></span><span class="op">(</span><span class="st">"Why is the sky blue?"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.1 ────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; The sky looks blue because of tiny particles in the air that bounce sunlight</span></span>
<span><span class="co">#&gt; around. Imagine throwing a ball off a cliff and watching it bounce on the</span></span>
<span><span class="co">#&gt; ground - the light from the sun does the same thing with these tiny particles,</span></span>
<span><span class="co">#&gt; making it look blue!</span></span></code></pre></div>
<p>By default, the package uses the “llama3.1 8B” model. Supported models can be found at <a href="https://ollama.com/library" class="external-link uri">https://ollama.com/library</a>. To download a specific model make use of the additional information available in “Tags” <a href="https://ollama.com/library/llama3.2/tags" class="external-link uri">https://ollama.com/library/llama3.2/tags</a>. Change this via <code>rollama_model</code>:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>rollama_model <span class="op">=</span> <span class="st">"llama3.2:3b-instruct-q4_1"</span><span class="op">)</span></span>
<span><span class="co"># if you don't have the model yet: pull_model("llama3.2:3b-instruct-q4_1")</span></span>
<span><span class="fu"><a href="reference/query.html">query</a></span><span class="op">(</span><span class="st">"Why is the sky blue? Answer with one sentence."</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.2:3b-instruct-q4_1 ───────────────────────────────────────</span></span>
<span><span class="co">#&gt; The Earth's sky looks blue because of something called light, which bounces off</span></span>
<span><span class="co">#&gt; tiny things in the air and comes back to us as blue!</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="easy-query-generation">Easy query generation<a class="anchor" aria-label="anchor" href="#easy-query-generation"></a>
</h2>
<p>The <code>make_query</code> function simplifies the creation of structured queries, which can, for example, be used in <a href="https://jbgruber.github.io/rollama/articles/annotation.html#the-make_query-helper-function">annotation tasks</a>.</p>
<p>Main components (check the <a href="https://jbgruber.github.io/rollama/articles/annotation.html#the-make_query-helper-function">documentation</a> for more options):</p>
<ul>
<li>
<strong><code>text</code></strong>: The text(s) to classify.</li>
<li>
<strong><code>prompt</code></strong>: Could be a (classification) question</li>
<li>
<strong><code>system</code></strong>: Optional system prompt providing context or instructions for the task.</li>
<li>
<strong><code>examples</code></strong>: Optional prior examples for one-shot or few-shot learning (user messages and assistant responses).</li>
</ul>
<p><strong>Zero-shot Example</strong><br>
In this example, the function is used without examples:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create a query using make_query</span></span>
<span><span class="va">q_zs</span> <span class="op">&lt;-</span> <span class="fu"><a href="reference/make_query.html">make_query</a></span><span class="op">(</span></span>
<span>  text <span class="op">=</span> <span class="st">"the pizza tastes terrible"</span>,</span>
<span>  prompt <span class="op">=</span> <span class="st">"Is this text: 'positive', 'neutral', or 'negative'?"</span>,</span>
<span>  system <span class="op">=</span> <span class="st">"You assign texts into categories. Answer with just the correct category."</span></span>
<span><span class="op">)</span></span>
<span><span class="co"># Print the query</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">q_zs</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; # A tibble: 2 × 2</span></span>
<span><span class="co">#&gt;   role   content                                                                </span></span>
<span><span class="co">#&gt;   &lt;chr&gt;  &lt;glue&gt;                                                                 </span></span>
<span><span class="co">#&gt; 1 system You assign texts into categories. Answer with just the correct categor…</span></span>
<span><span class="co">#&gt; 2 user   the pizza tastes terrible</span></span>
<span><span class="co">#&gt; Is this text: 'positive', 'neutral', or 'neg…</span></span>
<span><span class="co"># Run the query</span></span>
<span><span class="fu"><a href="reference/query.html">query</a></span><span class="op">(</span><span class="va">q_zs</span>, output <span class="op">=</span> <span class="st">"text"</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Answer from llama3.2:3b-instruct-q4_1 ───────────────────────────────────────</span></span>
<span><span class="co">#&gt; Negative</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="learn-more">Learn more<a class="anchor" aria-label="anchor" href="#learn-more"></a>
</h2>
<ul>
<li><a href="https://jbgruber.github.io/rollama/articles/annotation.html">Use rollama for annotation tasks</a></li>
<li><a href="https://jbgruber.github.io/rollama/articles/image-annotation.html">Annotate images</a></li>
<li><a href="https://jbgruber.github.io/rollama/articles/text-embedding.html">Get text embedding</a></li>
<li><a href="https://jbgruber.github.io/rollama/articles/hf-gguf.html">Use more models (GGUF format) from Hugging Face</a></li>
</ul>
</div>
<div class="section level2">
<h2 id="citation">Citation<a class="anchor" aria-label="anchor" href="#citation"></a>
</h2>
<p>Please cite the package using the <a href="https://arxiv.org/abs/2404.07654" class="external-link">pre print</a> DOI: <a href="https://doi.org/10.48550/arXiv.2404.07654" class="external-link uri">https://doi.org/10.48550/arXiv.2404.07654</a></p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=rollama" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/JBGruber/rollama/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/JBGruber/rollama/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="LICENSE.html">Full license</a></li>
<li><small>GPL (&gt;= 3)</small></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing rollama</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Johannes B. Gruber <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0001-9177-1772" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
<li>Maximilian Weber <br><small class="roles"> Author, contributor </small> <a href="https://orcid.org/0000-0002-1174-449X" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/JBGruber/rollama/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/JBGruber/rollama/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://app.codecov.io/gh/JBGruber/rollama?branch=main" class="external-link"><img src="https://codecov.io/gh/JBGruber/rollama/branch/main/graph/badge.svg" alt="Codecov test coverage"></a></li>
<li><a href="https://CRAN.R-project.org/package=rollama" class="external-link"><img src="https://www.r-pkg.org/badges/version/rollama" alt="CRAN status"></a></li>
<li><a href="https://cran.r-project.org/package=rollama" class="external-link"><img src="https://cranlogs.r-pkg.org/badges/grand-total/rollama" alt="CRAN_Download_Badge"></a></li>
<li><a href="https://doi.org/10.48550/arXiv.2404.07654" class="external-link"><img src="https://img.shields.io/badge/DOI-arXiv.2404.07654-B31B1B?logo=arxiv" alt="arXiv:10.48550/arXiv.2404.07654"></a></li>
<li><a href="https://saythanks.io/to/JBGruber" class="external-link"><img src="https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg" alt="say-thanks"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Johannes B. Gruber, Maximilian Weber.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
